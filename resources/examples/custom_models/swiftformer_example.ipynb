{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cdb4e1a-90d4-4625-ad75-75574b7189dc",
   "metadata": {},
   "source": [
    "# SwiftFormer pruning example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e47a0df-b89a-48b5-a11b-9abe0d9ac98b",
   "metadata": {},
   "source": [
    "In this toy example we are going to show how to run channel pruning on a custom model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "322f5ac8-fdcc-4958-bdab-d604813324dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01cdefa-7316-43a8-90cb-998382e5db6d",
   "metadata": {},
   "source": [
    "### 1. Download the code and checkpoint from https://github.com/Amshaker/SwiftFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0decfd14-4282-42b4-b5b6-3127bd017b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Amshaker/SwiftFormer.git\n",
    "!pip install timm==0.5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72225b0-0b00-4bdb-9a00-2a1fccba1cce",
   "metadata": {},
   "source": [
    "### 2. Load the model and check which modules can be dagable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eccb1811-099c-4e5a-a827-0c9f8a05e14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from SwiftFormer.models.swiftformer import SwiftFormer_S, EfficientAdditiveAttnetion\n",
    "import torch\n",
    "\n",
    "import torch_dag as td\n",
    "from torch_dag.commons import look_for_dagable_modules\n",
    "import torch_dag_algorithms as tda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def9ce01-1f6d-4b0e-980a-04bc23b0825f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.SwiftFormer'>\n",
      "/opt/conda/lib/python3.10/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/opt/conda/lib/python3.10/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/opt/conda/lib/python3.10/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/opt/conda/lib/python3.10/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "INFO:torch_dag.commons.debugging_tools:FAILURE: <class 'SwiftFormer.models.swiftformer.SwiftFormer'>\n",
      "INFO:torch_dag.commons.debugging_tools:Tensor type unknown to einops <class 'torch.fx.proxy.Proxy'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'torch.nn.modules.container.Sequential'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'torch.nn.modules.container.Sequential'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'torch.nn.modules.container.ModuleList'>\n",
      "INFO:torch_dag.commons.debugging_tools:FAILURE: <class 'torch.nn.modules.container.ModuleList'>\n",
      "INFO:torch_dag.commons.debugging_tools:Module [ModuleList] is missing the required \"forward\" function\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'torch.nn.modules.container.Sequential'>\n",
      "INFO:torch_dag.commons.debugging_tools:FAILURE: <class 'torch.nn.modules.container.Sequential'>\n",
      "INFO:torch_dag.commons.debugging_tools:Tensor type unknown to einops <class 'torch.fx.proxy.Proxy'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.SwiftFormerEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:FAILURE: <class 'SwiftFormer.models.swiftformer.SwiftFormerEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:Tensor type unknown to einops <class 'torch.fx.proxy.Proxy'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.SwiftFormerLocalRepresentation'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.SwiftFormerLocalRepresentation'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.EfficientAdditiveAttnetion'>\n",
      "INFO:torch_dag.commons.debugging_tools:FAILURE: <class 'SwiftFormer.models.swiftformer.EfficientAdditiveAttnetion'>\n",
      "INFO:torch_dag.commons.debugging_tools:Tensor type unknown to einops <class 'torch.fx.proxy.Proxy'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.Mlp'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.Mlp'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.Embedding'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.Embedding'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'torch.nn.modules.container.Sequential'>\n",
      "INFO:torch_dag.commons.debugging_tools:FAILURE: <class 'torch.nn.modules.container.Sequential'>\n",
      "INFO:torch_dag.commons.debugging_tools:Tensor type unknown to einops <class 'torch.fx.proxy.Proxy'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.SwiftFormerEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:FAILURE: <class 'SwiftFormer.models.swiftformer.SwiftFormerEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:Tensor type unknown to einops <class 'torch.fx.proxy.Proxy'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.SwiftFormerLocalRepresentation'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.SwiftFormerLocalRepresentation'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.EfficientAdditiveAttnetion'>\n",
      "INFO:torch_dag.commons.debugging_tools:FAILURE: <class 'SwiftFormer.models.swiftformer.EfficientAdditiveAttnetion'>\n",
      "INFO:torch_dag.commons.debugging_tools:Tensor type unknown to einops <class 'torch.fx.proxy.Proxy'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.Mlp'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.Mlp'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.Embedding'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.Embedding'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'torch.nn.modules.container.Sequential'>\n",
      "INFO:torch_dag.commons.debugging_tools:FAILURE: <class 'torch.nn.modules.container.Sequential'>\n",
      "INFO:torch_dag.commons.debugging_tools:Tensor type unknown to einops <class 'torch.fx.proxy.Proxy'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.SwiftFormerEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:FAILURE: <class 'SwiftFormer.models.swiftformer.SwiftFormerEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:Tensor type unknown to einops <class 'torch.fx.proxy.Proxy'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.SwiftFormerLocalRepresentation'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.SwiftFormerLocalRepresentation'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.EfficientAdditiveAttnetion'>\n",
      "INFO:torch_dag.commons.debugging_tools:FAILURE: <class 'SwiftFormer.models.swiftformer.EfficientAdditiveAttnetion'>\n",
      "INFO:torch_dag.commons.debugging_tools:Tensor type unknown to einops <class 'torch.fx.proxy.Proxy'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.Mlp'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.Mlp'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.Embedding'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.Embedding'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'torch.nn.modules.container.Sequential'>\n",
      "INFO:torch_dag.commons.debugging_tools:FAILURE: <class 'torch.nn.modules.container.Sequential'>\n",
      "INFO:torch_dag.commons.debugging_tools:Tensor type unknown to einops <class 'torch.fx.proxy.Proxy'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.SwiftFormerEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:FAILURE: <class 'SwiftFormer.models.swiftformer.SwiftFormerEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:Tensor type unknown to einops <class 'torch.fx.proxy.Proxy'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.SwiftFormerLocalRepresentation'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.SwiftFormerLocalRepresentation'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.EfficientAdditiveAttnetion'>\n",
      "INFO:torch_dag.commons.debugging_tools:FAILURE: <class 'SwiftFormer.models.swiftformer.EfficientAdditiveAttnetion'>\n",
      "INFO:torch_dag.commons.debugging_tools:Tensor type unknown to einops <class 'torch.fx.proxy.Proxy'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.Mlp'>\n",
      "INFO:torch_dag.commons.debugging_tools:SUCCESS: <class 'SwiftFormer.models.swiftformer.Mlp'>\n",
      "INFO:torch_dag.commons.debugging_tools:Dagable classes:\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.Mlp'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.ConvEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.Embedding'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'torch.nn.modules.container.Sequential'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.SwiftFormerLocalRepresentation'>\n",
      "INFO:torch_dag.commons.debugging_tools:Undagable classes:\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'torch.nn.modules.container.ModuleList'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.SwiftFormer'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'torch.nn.modules.container.Sequential'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.SwiftFormerEncoder'>\n",
      "INFO:torch_dag.commons.debugging_tools:<class 'SwiftFormer.models.swiftformer.EfficientAdditiveAttnetion'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({SwiftFormer.models.swiftformer.ConvEncoder,\n",
       "  SwiftFormer.models.swiftformer.Embedding,\n",
       "  SwiftFormer.models.swiftformer.Mlp,\n",
       "  SwiftFormer.models.swiftformer.SwiftFormerLocalRepresentation,\n",
       "  torch.nn.modules.container.Sequential},\n",
       " {SwiftFormer.models.swiftformer.EfficientAdditiveAttnetion,\n",
       "  SwiftFormer.models.swiftformer.SwiftFormer,\n",
       "  SwiftFormer.models.swiftformer.SwiftFormerEncoder,\n",
       "  torch.nn.modules.container.ModuleList,\n",
       "  torch.nn.modules.container.Sequential})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SwiftFormer_S()\n",
    "checkpoint_path = '/path/to/checkpoint'\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model'], strict=False)\n",
    "look_for_dagable_modules(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5b44ab-1979-4d6e-a6cb-ec465290f3c8",
   "metadata": {},
   "source": [
    "As we can see we cannot trace the EfficientAdditiveAttnetion, so we have to add it as exception to the build_from_unstructured_module function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a98965-ea3a-48de-b8b3-15c9c61bcdc1",
   "metadata": {},
   "source": [
    "### 3. Convert the model to DagModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8dd08a1-7640-4a25-9e24-13dcb1e408e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch_dag.core.module_handling:The module: EfficientAdditiveAttnetion(\n",
      "  (to_query): Linear(in_features=48, out_features=48, bias=True)\n",
      "  (to_key): Linear(in_features=48, out_features=48, bias=True)\n",
      "  (Proj): Linear(in_features=48, out_features=48, bias=True)\n",
      "  (final): Linear(in_features=48, out_features=48, bias=True)\n",
      ") of type: <class 'SwiftFormer.models.swiftformer.EfficientAdditiveAttnetion'> is not covered by `torch-dag`. by the DagModule. In particular, pruning support is not guaranteed.\n",
      "WARNING:torch_dag.core.module_handling:The module: EfficientAdditiveAttnetion(\n",
      "  (to_query): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (to_key): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (Proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (final): Linear(in_features=64, out_features=64, bias=True)\n",
      ") of type: <class 'SwiftFormer.models.swiftformer.EfficientAdditiveAttnetion'> is not covered by `torch-dag`. by the DagModule. In particular, pruning support is not guaranteed.\n",
      "WARNING:torch_dag.core.module_handling:The module: EfficientAdditiveAttnetion(\n",
      "  (to_query): Linear(in_features=168, out_features=168, bias=True)\n",
      "  (to_key): Linear(in_features=168, out_features=168, bias=True)\n",
      "  (Proj): Linear(in_features=168, out_features=168, bias=True)\n",
      "  (final): Linear(in_features=168, out_features=168, bias=True)\n",
      ") of type: <class 'SwiftFormer.models.swiftformer.EfficientAdditiveAttnetion'> is not covered by `torch-dag`. by the DagModule. In particular, pruning support is not guaranteed.\n",
      "WARNING:torch_dag.core.module_handling:The module: EfficientAdditiveAttnetion(\n",
      "  (to_query): Linear(in_features=224, out_features=224, bias=True)\n",
      "  (to_key): Linear(in_features=224, out_features=224, bias=True)\n",
      "  (Proj): Linear(in_features=224, out_features=224, bias=True)\n",
      "  (final): Linear(in_features=224, out_features=224, bias=True)\n",
      ") of type: <class 'SwiftFormer.models.swiftformer.EfficientAdditiveAttnetion'> is not covered by `torch-dag`. by the DagModule. In particular, pruning support is not guaranteed.\n"
     ]
    }
   ],
   "source": [
    "custom_module_classes = (EfficientAdditiveAttnetion,)\n",
    "INPUT_SHAPE = (1, 3, 224, 224)\n",
    "PRUNING_PROPORTION = 0.5\n",
    "NUM_PRUNING_STEPS = 100_000\n",
    "dag = td.build_from_unstructured_module(\n",
    "    model,\n",
    "    custom_autowrap_torch_module_classes=custom_module_classes,\n",
    ")\n",
    "td.compare_module_outputs(first_module=model, second_module=dag, input_shape=INPUT_SHAPE) # sanity check for conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6bd39b-f626-45b3-ad69-1966df7c97e5",
   "metadata": {},
   "source": [
    "### 5. Prepare the converted model for pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "134a4bc3-0d49-4ae7-963a-24b06029c291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch_dag_algorithms.pruning.filters:[\u001b[1m\u001b[96mNonPrunableCustomModulesFilter\u001b[0m] Removing orbit \u001b[1m\u001b[95mOrbit\u001b[0m[\u001b[1m\u001b[93mcolor\u001b[0m=2, \u001b[1m\u001b[93mdiscovery_stage\u001b[0m=OrbitsDiscoveryStage.EXTENDED_ORBIT_DISCOVERY, \u001b[1m\u001b[93msources\u001b[0m=[patch_embed_3, network_0_0_pwconv2, network_0_1_pwconv2, network_0_2_local_representation_pwconv2, network_0_2_linear_fc2], \u001b[1m\u001b[93msinks\u001b[0m=[network_0_0_pwconv1, network_0_1_pwconv1, network_0_2_local_representation_pwconv1, network_0_2_linear_fc1, network_1_proj], \u001b[1m\u001b[93mnon_border\u001b[0m={permute, mul, mul_3, add, reshape, network_0_1_dwconv, network_0_2_attn, network_0_1_norm, network_0_1_layer_scale, reshape_1, network_0_2_local_representation_layer_scale, permute_1, mul_4, network_0_2_layer_scale_1, add_3, network_0_2_layer_scale_2, mul_1, network_0_2_linear_norm1, add_1, network_0_2_local_representation_dwconv, network_0_2_local_representation_norm, network_0_2_linear_drop_1, mul_5, network_0_0_layer_scale, mul_2, patch_embed_4, add_4, add_2, patch_embed_5, getattr_1, network_0_0_dwconv, getitem, network_0_0_norm, getitem_1, getitem_2, getitem_3}, \u001b[1m\u001b[93mend_path\u001b[0m=[(network_0_0_norm, network_0_0_pwconv1), (network_0_1_norm, network_0_1_pwconv1), (network_0_2_local_representation_norm, network_0_2_local_representation_pwconv1), (network_0_2_linear_norm1, network_0_2_linear_fc1), (add_4, network_1_proj)]]\n",
      "INFO:torch_dag_algorithms.pruning.filters:[\u001b[1m\u001b[96mNonPrunableCustomModulesFilter\u001b[0m] Removing orbit \u001b[1m\u001b[95mOrbit\u001b[0m[\u001b[1m\u001b[93mcolor\u001b[0m=7, \u001b[1m\u001b[93mdiscovery_stage\u001b[0m=OrbitsDiscoveryStage.EXTENDED_ORBIT_DISCOVERY, \u001b[1m\u001b[93msources\u001b[0m=[network_1_proj, network_2_0_pwconv2, network_2_1_pwconv2, network_2_2_local_representation_pwconv2, network_2_2_linear_fc2], \u001b[1m\u001b[93msinks\u001b[0m=[network_2_0_pwconv1, network_2_1_pwconv1, network_2_2_local_representation_pwconv1, network_2_2_linear_fc1, network_3_proj], \u001b[1m\u001b[93mnon_border\u001b[0m={getitem_7, mul_6, permute_2, add_5, mul_9, network_2_1_dwconv, reshape_2, network_2_1_norm, network_2_2_attn, reshape_3, permute_3, network_2_0_layer_scale, mul_10, mul_7, network_2_1_layer_scale, add_8, add_6, network_2_2_local_representation_layer_scale, network_2_2_linear_norm1, network_2_2_local_representation_dwconv, network_2_2_layer_scale_1, network_2_2_local_representation_norm, network_2_2_layer_scale_2, network_2_2_linear_drop_1, mul_8, mul_11, add_7, network_1_norm, add_9, getattr_2, network_2_0_dwconv, getitem_4, network_2_0_norm, getitem_5, getitem_6}, \u001b[1m\u001b[93mend_path\u001b[0m=[(network_2_0_norm, network_2_0_pwconv1), (network_2_1_norm, network_2_1_pwconv1), (network_2_2_local_representation_norm, network_2_2_local_representation_pwconv1), (network_2_2_linear_norm1, network_2_2_linear_fc1), (add_9, network_3_proj)]]\n",
      "INFO:torch_dag_algorithms.pruning.filters:[\u001b[1m\u001b[96mNonPrunableCustomModulesFilter\u001b[0m] Removing orbit \u001b[1m\u001b[95mOrbit\u001b[0m[\u001b[1m\u001b[93mcolor\u001b[0m=12, \u001b[1m\u001b[93mdiscovery_stage\u001b[0m=OrbitsDiscoveryStage.EXTENDED_ORBIT_DISCOVERY, \u001b[1m\u001b[93msources\u001b[0m=[network_3_proj, network_4_0_pwconv2, network_4_1_pwconv2, network_4_2_pwconv2, network_4_3_pwconv2, network_4_4_pwconv2, network_4_5_pwconv2, network_4_6_pwconv2, network_4_7_pwconv2, network_4_8_local_representation_pwconv2, network_4_8_linear_fc2], \u001b[1m\u001b[93msinks\u001b[0m=[network_4_0_pwconv1, network_4_1_pwconv1, network_4_2_pwconv1, network_4_3_pwconv1, network_4_4_pwconv1, network_4_5_pwconv1, network_4_6_pwconv1, network_4_7_pwconv1, network_4_8_local_representation_pwconv1, network_4_8_linear_fc1, network_5_proj], \u001b[1m\u001b[93mnon_border\u001b[0m={network_4_8_layer_scale_1, getitem_10, network_4_8_layer_scale_2, getitem_11, network_4_7_layer_scale, permute_4, mul_18, mul_15, mul_21, mul_12, add_16, add_13, reshape_4, add_10, network_4_7_dwconv, network_4_4_dwconv, network_4_8_attn, network_4_1_dwconv, network_4_7_norm, network_4_4_norm, reshape_5, network_4_1_norm, network_4_8_local_representation_layer_scale, getitem_8, permute_5, mul_22, add_19, mul_19, mul_16, network_4_8_linear_norm1, mul_13, add_17, add_14, add_11, network_4_8_local_representation_dwconv, network_4_5_dwconv, getitem_9, network_4_2_dwconv, network_4_0_layer_scale, network_4_8_local_representation_norm, network_4_5_norm, network_4_2_norm, network_4_1_layer_scale, network_4_2_layer_scale, network_4_8_linear_drop_1, network_4_3_layer_scale, mul_23, network_4_4_layer_scale, mul_20, mul_17, add_20, mul_14, network_4_5_layer_scale, add_18, add_15, add_12, network_3_norm, network_4_6_layer_scale, network_4_6_dwconv, getattr_3, network_4_3_dwconv, network_4_0_dwconv, network_4_6_norm, network_4_3_norm, network_4_0_norm}, \u001b[1m\u001b[93mend_path\u001b[0m=[(network_4_0_norm, network_4_0_pwconv1), (network_4_1_norm, network_4_1_pwconv1), (network_4_2_norm, network_4_2_pwconv1), (network_4_3_norm, network_4_3_pwconv1), (network_4_4_norm, network_4_4_pwconv1), (network_4_5_norm, network_4_5_pwconv1), (network_4_6_norm, network_4_6_pwconv1), (network_4_7_norm, network_4_7_pwconv1), (network_4_8_local_representation_norm, network_4_8_local_representation_pwconv1), (network_4_8_linear_norm1, network_4_8_linear_fc1), (add_20, network_5_proj)]]\n",
      "INFO:torch_dag_algorithms.pruning.filters:[\u001b[1m\u001b[96mNonPrunableCustomModulesFilter\u001b[0m] Removing orbit \u001b[1m\u001b[95mOrbit\u001b[0m[\u001b[1m\u001b[93mcolor\u001b[0m=23, \u001b[1m\u001b[93mdiscovery_stage\u001b[0m=OrbitsDiscoveryStage.EXTENDED_ORBIT_DISCOVERY, \u001b[1m\u001b[93msources\u001b[0m=[network_5_proj, network_6_0_pwconv2, network_6_1_pwconv2, network_6_2_pwconv2, network_6_3_pwconv2, network_6_4_pwconv2, network_6_5_local_representation_pwconv2, network_6_5_linear_fc2], \u001b[1m\u001b[93msinks\u001b[0m=[network_6_0_pwconv1, network_6_1_pwconv1, network_6_2_pwconv1, network_6_3_pwconv1, network_6_4_pwconv1, network_6_5_local_representation_pwconv1, network_6_5_linear_fc1, head, dist_head], \u001b[1m\u001b[93mnon_border\u001b[0m={network_6_3_norm, network_6_0_layer_scale, network_6_1_layer_scale, mul_27, network_6_2_layer_scale, add_24, network_6_3_layer_scale, network_6_4_dwconv, network_6_4_layer_scale, network_6_4_norm, network_6_5_local_representation_layer_scale, network_6_5_layer_scale_1, network_6_5_layer_scale_2, mul_28, add_25, network_6_5_local_representation_dwconv, network_6_5_local_representation_norm, mul_29, add_26, network_5_norm, getattr_4, network_6_0_dwconv, getitem_12, network_6_0_norm, getitem_13, getitem_14, getitem_15, permute_6, mul_24, mul_30, add_21, reshape_6, network_6_1_dwconv, network_6_5_attn, network_6_1_norm, reshape_7, permute_7, mul_31, add_27, network_6_5_linear_norm1, mul_25, add_22, network_6_2_dwconv, network_6_5_linear_drop_1, mul_32, network_6_2_norm, add_28, norm, flatten, mean, flatten_1, mean_1, mul_26, add_23, network_6_3_dwconv}, \u001b[1m\u001b[93mend_path\u001b[0m=[(network_6_0_norm, network_6_0_pwconv1), (network_6_1_norm, network_6_1_pwconv1), (network_6_2_norm, network_6_2_pwconv1), (network_6_3_norm, network_6_3_pwconv1), (network_6_4_norm, network_6_4_pwconv1), (network_6_5_local_representation_norm, network_6_5_local_representation_pwconv1), (network_6_5_linear_norm1, network_6_5_linear_fc1), (mean, head), (mean_1, dist_head)]]\n",
      "INFO:torch_dag_algorithms.pruning.filters:[\u001b[1m\u001b[96mOutputInScopeFilter\u001b[0m] Removing orbit \u001b[1m\u001b[95mOrbit\u001b[0m[\u001b[1m\u001b[93mcolor\u001b[0m=31, \u001b[1m\u001b[93mdiscovery_stage\u001b[0m=OrbitsDiscoveryStage.EXTENDED_ORBIT_DISCOVERY, \u001b[1m\u001b[93msources\u001b[0m=[head, dist_head], \u001b[1m\u001b[93msinks\u001b[0m=[], \u001b[1m\u001b[93mnon_border\u001b[0m={add_29, truediv}, \u001b[1m\u001b[93mend_path\u001b[0m=[]]\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:[+] Total normalized flops: 39.433783920599545\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:[+] Prunable normalized flops: 33.688999999999986\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:[+] Unprunable normalized flops: 5.744783920599559\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:[+] Normalized flops per orbit:\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=32]: normalized flops=1.66\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=33]: normalized flops=2.30\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=34]: normalized flops=2.30\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=35]: normalized flops=0.58\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=36]: normalized flops=2.30\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=37]: normalized flops=1.02\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=38]: normalized flops=1.02\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=39]: normalized flops=0.26\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=40]: normalized flops=1.02\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=41]: normalized flops=1.76\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=42]: normalized flops=1.76\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=43]: normalized flops=1.76\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=44]: normalized flops=1.76\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=45]: normalized flops=1.76\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=46]: normalized flops=1.76\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=47]: normalized flops=1.76\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=48]: normalized flops=1.76\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=49]: normalized flops=0.44\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=50]: normalized flops=1.76\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=51]: normalized flops=0.78\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=52]: normalized flops=0.78\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=53]: normalized flops=0.78\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=54]: normalized flops=0.78\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=55]: normalized flops=0.78\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=56]: normalized flops=0.20\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=57]: normalized flops=0.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prunable proportion: 0.8543182177960208\n"
     ]
    }
   ],
   "source": [
    "pruning_config = tda.pruning.ChannelPruning(\n",
    "    model=dag,\n",
    "    input_shape_without_batch=INPUT_SHAPE[1:],\n",
    "    pruning_proportion=PRUNING_PROPORTION,\n",
    "    num_training_steps=NUM_PRUNING_STEPS,\n",
    "    anneal_losses=False,\n",
    "    custom_unprunable_module_classes=custom_module_classes\n",
    ")\n",
    "pruning_model = pruning_config.prepare_for_pruning()\n",
    "\n",
    "print(f'Prunable proportion: {pruning_config.prunable_proportion}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff13dd3-441c-4f82-b61d-ae626c8e15d2",
   "metadata": {},
   "source": [
    "### 6. Run trainnig with pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a905cab-f377-4df5-a62d-1f59f27ca52b",
   "metadata": {},
   "source": [
    "Now we should train the model, but to save time we will just set logits in orbits to random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19f07628-fc86-44dc-9482-398a64387592",
   "metadata": {},
   "outputs": [],
   "source": [
    "orbits_dict = tda.pruning.get_orbits_dict(pruning_model)\n",
    "for k, v in orbits_dict.items():\n",
    "    num_channels = v.num_channels\n",
    "    v.debug_logits = torch.normal(mean=torch.zeros(size=(num_channels,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67346cd0-f1a7-4305-9dde-e9d8b87ef99f",
   "metadata": {},
   "source": [
    "### 7. Remove channels from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a003734-645f-4c43-9ba1-78ff61afc173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv patch_embed_0: leaving fraction: 0.4583333333333333 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv patch_embed_3: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_0_0_pwconv1: leaving fraction: 0.4895833333333333 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_0_0_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_0_1_pwconv1: leaving fraction: 0.5104166666666666 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_0_1_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_0_2_local_representation_pwconv1: leaving fraction: 0.5208333333333334 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_0_2_local_representation_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.mask_propagation:No explicit mask propagation for vertex: getattr_1, of type <class 'torch_dag.structured_modules.GetShapeModuleV2'>. Returning `None` masks.\n",
      "INFO:torch_dag_algorithms.pruning.mask_propagation:No explicit mask propagation for vertex: permute, of type <class 'torch_dag.structured_modules.PermuteModule'>. Returning `None` masks.\n",
      "WARNING:torch_dag_algorithms.pruning.channel_removal_primitives:No explicit channel removal logic implemented for <class 'SwiftFormer.models.swiftformer.EfficientAdditiveAttnetion'>. Pruning will return the module as it is. In case of errors make sure that the module can handle variable number of input channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_0_2_linear_fc1: leaving fraction: 0.5364583333333334 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_0_2_linear_fc2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_1_proj: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_2_0_pwconv1: leaving fraction: 0.49609375 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_2_0_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_2_1_pwconv1: leaving fraction: 0.51171875 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_2_1_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_2_2_local_representation_pwconv1: leaving fraction: 0.59375 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_2_2_local_representation_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.mask_propagation:No explicit mask propagation for vertex: getattr_2, of type <class 'torch_dag.structured_modules.GetShapeModuleV2'>. Returning `None` masks.\n",
      "INFO:torch_dag_algorithms.pruning.mask_propagation:No explicit mask propagation for vertex: permute_2, of type <class 'torch_dag.structured_modules.PermuteModule'>. Returning `None` masks.\n",
      "WARNING:torch_dag_algorithms.pruning.channel_removal_primitives:No explicit channel removal logic implemented for <class 'SwiftFormer.models.swiftformer.EfficientAdditiveAttnetion'>. Pruning will return the module as it is. In case of errors make sure that the module can handle variable number of input channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_2_2_linear_fc1: leaving fraction: 0.515625 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_2_2_linear_fc2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_3_proj: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_4_0_pwconv1: leaving fraction: 0.5372023809523809 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_4_0_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_4_1_pwconv1: leaving fraction: 0.4895833333333333 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_4_1_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_4_2_pwconv1: leaving fraction: 0.4642857142857143 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_4_2_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_4_3_pwconv1: leaving fraction: 0.5416666666666666 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_4_3_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_4_4_pwconv1: leaving fraction: 0.5089285714285714 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_4_4_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_4_5_pwconv1: leaving fraction: 0.5148809523809523 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_4_5_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_4_6_pwconv1: leaving fraction: 0.5252976190476191 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_4_6_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_4_7_pwconv1: leaving fraction: 0.5029761904761905 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_4_7_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_4_8_local_representation_pwconv1: leaving fraction: 0.47619047619047616 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_4_8_local_representation_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.mask_propagation:No explicit mask propagation for vertex: getattr_3, of type <class 'torch_dag.structured_modules.GetShapeModuleV2'>. Returning `None` masks.\n",
      "INFO:torch_dag_algorithms.pruning.mask_propagation:No explicit mask propagation for vertex: permute_4, of type <class 'torch_dag.structured_modules.PermuteModule'>. Returning `None` masks.\n",
      "WARNING:torch_dag_algorithms.pruning.channel_removal_primitives:No explicit channel removal logic implemented for <class 'SwiftFormer.models.swiftformer.EfficientAdditiveAttnetion'>. Pruning will return the module as it is. In case of errors make sure that the module can handle variable number of input channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_4_8_linear_fc1: leaving fraction: 0.49404761904761907 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_4_8_linear_fc2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_5_proj: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_6_0_pwconv1: leaving fraction: 0.5033482142857143 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_6_0_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_6_1_pwconv1: leaving fraction: 0.5078125 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_6_1_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_6_2_pwconv1: leaving fraction: 0.5357142857142857 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_6_2_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_6_3_pwconv1: leaving fraction: 0.4955357142857143 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_6_3_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_6_4_pwconv1: leaving fraction: 0.49888392857142855 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_6_4_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_6_5_local_representation_pwconv1: leaving fraction: 0.5357142857142857 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_6_5_local_representation_pwconv2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.mask_propagation:No explicit mask propagation for vertex: getattr_4, of type <class 'torch_dag.structured_modules.GetShapeModuleV2'>. Returning `None` masks.\n",
      "INFO:torch_dag_algorithms.pruning.mask_propagation:No explicit mask propagation for vertex: permute_6, of type <class 'torch_dag.structured_modules.PermuteModule'>. Returning `None` masks.\n",
      "WARNING:torch_dag_algorithms.pruning.channel_removal_primitives:No explicit channel removal logic implemented for <class 'SwiftFormer.models.swiftformer.EfficientAdditiveAttnetion'>. Pruning will return the module as it is. In case of errors make sure that the module can handle variable number of input channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_6_5_linear_fc1: leaving fraction: 0.53125 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv network_6_5_linear_fc2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.mask_propagation:No explicit mask propagation for vertex: flatten, of type <class 'torch_dag.structured_modules.FlattenModule'>. Returning `None` masks.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv head: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.mask_propagation:No explicit mask propagation for vertex: flatten_1, of type <class 'torch_dag.structured_modules.FlattenModule'>. Returning `None` masks.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv dist_head: leaving fraction: 1.0 of out channels.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5796244146040351\n"
     ]
    }
   ],
   "source": [
    "pre_kmapp = td.commons.compute_static_kmapp(pruning_model, input_shape_without_batch=INPUT_SHAPE[1:])\n",
    "\n",
    "dag_final = tda.pruning.remove_channels_in_dag(pruning_model, input_shape=(1, 3, 224, 224))\n",
    "post_kmapp = td.commons.compute_static_kmapp(dag_final, input_shape_without_batch=INPUT_SHAPE[1:])\n",
    "\n",
    "print(post_kmapp/ pre_kmapp)\n",
    "\n",
    "dag_final.save('/path/to/model/pruned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3345da25-5a6b-4030-bd1d-bc177247d8c8",
   "metadata": {},
   "source": [
    "### 8. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b590f1-949d-4b47-8652-9bd8ddbe15a6",
   "metadata": {},
   "source": [
    "|        Model         | Accuracy | kmapps |\n",
    "|:--------------------:|:--------:| ------ |\n",
    "|    SwiftFormer-S     |  78.5%   | 39.43  |\n",
    "|    SwiftFormer-XS    |  75.7%   | 24.18  |\n",
    "| SwiftFormer-S-pruned |  77.1%  | 19.96  |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
