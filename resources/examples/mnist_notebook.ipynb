{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "790ba30f-41f3-4dc7-b0c7-b3643e4b697b",
   "metadata": {},
   "source": [
    "# Toy pruning example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c61ee-0549-4626-a25b-2b8eae82ecc4",
   "metadata": {},
   "source": [
    "> NOTE: The example below is for educational purposes only. One can easily compress the model we define below by just reducing the number of channels manually. The purpose is to show how to integrate channel pruning into a `torch` training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c9b56b-3d89-4cbe-882f-a63062dc9c72",
   "metadata": {},
   "source": [
    "In this toy example we are going to show how to run channel pruning on a really simple model on `MNIST` dataset.\n",
    "Even though, its just `MNIST` the same workflow and principles apply to running channel pruning with `torch-dag` on other models. The outline of the notebook is as follows:\n",
    "1. Download the data.\n",
    "2. Build a `torch.nn.Module` model.\n",
    "3. Train it and compute accuracy on the test set.\n",
    "4. Convert the model to `torch-dag` `DagModule` format.\n",
    "5. Prepare the converted model for pruning.\n",
    "6. Run trainnig with pruning.\n",
    "7. Remove channels from the model.\n",
    "8. Report accuracy after pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d410d66-b7c0-4607-8116-7c2c06a4df82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95c9281e-fbe7-48e8-9b55-0ee1dd129306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "818ed6be-001f-46b0-a5f3-a786b9807119",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "CPU_DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ea9dc8-51b8-4bb4-8240-32af4b7c3adc",
   "metadata": {},
   "source": [
    "## 2. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e028a367-337c-4ae7-9b0e-36e29705af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the convolutional model\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.conv1(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = self.activation(self.conv3(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return nn.functional.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52f9080-32f5-45a0-b8d5-4fdd5761a01b",
   "metadata": {},
   "source": [
    "## 3. Train the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46dcf13a-6323-417c-8e98-4621607051e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and optimizer\n",
    "model = ConvNet()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e4b8088-2898-44fd-bfda-08d1454bd2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/600], Loss: 0.2511\n",
      "Epoch [1/2], Step [200/600], Loss: 0.2141\n",
      "Epoch [1/2], Step [300/600], Loss: 0.1352\n",
      "Epoch [1/2], Step [400/600], Loss: 0.0589\n",
      "Epoch [1/2], Step [500/600], Loss: 0.0338\n",
      "Epoch [1/2], Step [600/600], Loss: 0.0627\n",
      "Epoch [2/2], Step [100/600], Loss: 0.0257\n",
      "Epoch [2/2], Step [200/600], Loss: 0.0297\n",
      "Epoch [2/2], Step [300/600], Loss: 0.0430\n",
      "Epoch [2/2], Step [400/600], Loss: 0.0793\n",
      "Epoch [2/2], Step [500/600], Loss: 0.0444\n",
      "Epoch [2/2], Step [600/600], Loss: 0.0675\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model.to(DEVICE)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = nn.functional.nll_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, len(train_loader),\n",
    "                                                                     loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3af53c1b-eeb7-4112-9906-5995f0627d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 98.77 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebee6834-f7f0-4953-a954-f185c420a734",
   "metadata": {},
   "source": [
    "## 4. Convert to `DagModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dbeb035-a83a-4633-b0c2-849f397db5a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/opt/conda/lib/python3.10/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/opt/conda/lib/python3.10/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/opt/conda/lib/python3.10/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "WARNING:torch_dag.core.module_handling:The module: LogSoftmax(dim=1) of type: <class 'torch.nn.modules.activation.LogSoftmax'> is not covered by `torch-dag`. by the DagModule. In particular, pruning support is not guaranteed.\n"
     ]
    }
   ],
   "source": [
    "import torch_dag as td\n",
    "import torch_dag_algorithms as tda\n",
    "model.to(CPU_DEVICE)\n",
    "dag = td.build_from_unstructured_module(model)\n",
    "td.compare_module_outputs(first_module=model, second_module=dag, input_shape=(8, 1, 28, 28)) # sanity check for conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741251be-61f4-40c5-a102-40322f94db36",
   "metadata": {},
   "source": [
    "## 5. Prepare the converted model for pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d69660b-cc6d-4bc0-8bcd-7760106a85d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch_dag_algorithms.pruning.filters:[\u001b[1m\u001b[96mNonPrunableCustomModulesFilter\u001b[0m] Removing orbit \u001b[1m\u001b[95mOrbit\u001b[0m[\u001b[1m\u001b[93mcolor\u001b[0m=3, \u001b[1m\u001b[93mdiscovery_stage\u001b[0m=OrbitsDiscoveryStage.EXTENDED_ORBIT_DISCOVERY, \u001b[1m\u001b[93msources\u001b[0m=[conv3], \u001b[1m\u001b[93msinks\u001b[0m=[fc1], \u001b[1m\u001b[93mnon_border\u001b[0m={flatten, activation_2}, \u001b[1m\u001b[93mend_path\u001b[0m=[(flatten, fc1)]]\n",
      "INFO:torch_dag_algorithms.pruning.filters:[\u001b[1m\u001b[96mOutputInScopeFilter\u001b[0m] Removing orbit \u001b[1m\u001b[95mOrbit\u001b[0m[\u001b[1m\u001b[93mcolor\u001b[0m=6, \u001b[1m\u001b[93mdiscovery_stage\u001b[0m=OrbitsDiscoveryStage.EXTENDED_ORBIT_DISCOVERY, \u001b[1m\u001b[93msources\u001b[0m=[fc3], \u001b[1m\u001b[93msinks\u001b[0m=[], \u001b[1m\u001b[93mnon_border\u001b[0m={log_softmax}, \u001b[1m\u001b[93mend_path\u001b[0m=[]]\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:[+] Total normalized flops: 21.232244897959177\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:[+] Prunable normalized flops: 21.21624489795918\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:[+] Unprunable normalized flops: 0.015999999999998238\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:[+] Normalized flops per orbit:\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=7]: normalized flops=0.67\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=8]: normalized flops=18.48\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=9]: normalized flops=2.05\n",
      "INFO:torch_dag_algorithms.pruning.dag_orbitalizer:Orbit[color=10]: normalized flops=0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prunable proportion: 0.9992464291893347\n"
     ]
    }
   ],
   "source": [
    "INPUT_SHAPE = (1, 1, 28, 28)\n",
    "PRUNING_PROPORTION = 0.5  # target model size relative to the original model\n",
    "NUM_PRUNING_STEPS = 5000\n",
    "batch_size = 100\n",
    "initial_normalized_flops = tda.pruning.compute_normalized_flops(dag, input_shape_without_batch=INPUT_SHAPE[1:])\n",
    "\n",
    "pruning_config = tda.pruning.ChannelPruning(\n",
    "    model=dag,\n",
    "    input_shape_without_batch=INPUT_SHAPE[1:],\n",
    "    pruning_proportion=PRUNING_PROPORTION,\n",
    "    num_training_steps=NUM_PRUNING_STEPS,\n",
    "    anneal_losses=False,\n",
    ")\n",
    "\n",
    "pruning_model = pruning_config.prepare_for_pruning()\n",
    "print(f'Prunable proportion: {pruning_config.prunable_proportion}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df986895-509d-423b-9f76-2ddb3d8f8132",
   "metadata": {},
   "source": [
    "## 6. Run trainnig with pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05a164eb-9232-406e-a481-f04420af6362",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(pruning_model.parameters(), lr=0.001)\n",
    "\n",
    "global_step = 0\n",
    "batches_per_epoch = len(train_loader)\n",
    "num_epochs = NUM_PRUNING_STEPS // batches_per_epoch\n",
    "\n",
    "_ = pruning_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c42a6dd8-96aa-4981-bfef-dc5f46e0a727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step: 100/5000, proportion: 0.9245241284370422, task loss: 0.15911529958248138, entropy loss: 0.20658813416957855\n",
      "global step: 200/5000, proportion: 0.8771666884422302, task loss: 0.10889792442321777, entropy loss: 0.2939905524253845\n",
      "global step: 300/5000, proportion: 0.6751950979232788, task loss: 0.0686008483171463, entropy loss: 0.38392454385757446\n",
      "global step: 400/5000, proportion: 0.4627421498298645, task loss: 0.033269405364990234, entropy loss: 0.39139097929000854\n",
      "global step: 500/5000, proportion: 0.4674898386001587, task loss: 0.15927720069885254, entropy loss: 0.38195693492889404\n",
      "global step: 600/5000, proportion: 0.47195300459861755, task loss: 0.04428831487894058, entropy loss: 0.3704186677932739\n",
      "global step: 700/5000, proportion: 0.4754504859447479, task loss: 0.08102502673864365, entropy loss: 0.35771644115448\n",
      "global step: 800/5000, proportion: 0.4793959856033325, task loss: 0.11654949188232422, entropy loss: 0.34719157218933105\n",
      "global step: 900/5000, proportion: 0.4832599461078644, task loss: 0.0372016504406929, entropy loss: 0.33835452795028687\n",
      "global step: 1000/5000, proportion: 0.4877482056617737, task loss: 0.02087351493537426, entropy loss: 0.330794095993042\n",
      "global step: 1100/5000, proportion: 0.49226415157318115, task loss: 0.059686288237571716, entropy loss: 0.3228854238986969\n",
      "global step: 1200/5000, proportion: 0.4976259469985962, task loss: 0.03504035621881485, entropy loss: 0.3171500563621521\n",
      "global step: 1300/5000, proportion: 0.4906204044818878, task loss: 0.03946543484926224, entropy loss: 0.31030750274658203\n",
      "global step: 1400/5000, proportion: 0.49477019906044006, task loss: 0.11787659674882889, entropy loss: 0.3037335276603699\n",
      "global step: 1500/5000, proportion: 0.4996207654476166, task loss: 0.008622347377240658, entropy loss: 0.2979413866996765\n",
      "global step: 1600/5000, proportion: 0.4905737340450287, task loss: 0.005910657811909914, entropy loss: 0.293111115694046\n",
      "global step: 1700/5000, proportion: 0.49484366178512573, task loss: 0.036374639719724655, entropy loss: 0.28706181049346924\n",
      "global step: 1800/5000, proportion: 0.49853596091270447, task loss: 0.0020511867478489876, entropy loss: 0.280902624130249\n",
      "global step: 1900/5000, proportion: 0.48897677659988403, task loss: 0.052519846707582474, entropy loss: 0.27563533186912537\n",
      "global step: 2000/5000, proportion: 0.4938655495643616, task loss: 0.053677383810281754, entropy loss: 0.27055761218070984\n",
      "global step: 2100/5000, proportion: 0.49919989705085754, task loss: 0.14902426302433014, entropy loss: 0.26553747057914734\n",
      "global step: 2200/5000, proportion: 0.48578891158103943, task loss: 0.1632584184408188, entropy loss: 0.2611693739891052\n",
      "global step: 2300/5000, proportion: 0.4900078773498535, task loss: 0.17372074723243713, entropy loss: 0.2568598985671997\n",
      "global step: 2400/5000, proportion: 0.4949912428855896, task loss: 0.023807596415281296, entropy loss: 0.2528064548969269\n",
      "global step: 2500/5000, proportion: 0.48483705520629883, task loss: 0.06758172810077667, entropy loss: 0.24932685494422913\n",
      "global step: 2600/5000, proportion: 0.4845214784145355, task loss: 0.003712062258273363, entropy loss: 0.24595138430595398\n",
      "global step: 2700/5000, proportion: 0.4890735149383545, task loss: 0.017809322103857994, entropy loss: 0.2427986115217209\n",
      "global step: 2800/5000, proportion: 0.49422359466552734, task loss: 0.015636557713150978, entropy loss: 0.23993191123008728\n",
      "global step: 2900/5000, proportion: 0.499795138835907, task loss: 0.13440290093421936, entropy loss: 0.23737174272537231\n",
      "global step: 3000/5000, proportion: 0.48140567541122437, task loss: 0.018275408074259758, entropy loss: 0.23557482659816742\n",
      "global step: 3100/5000, proportion: 0.48494091629981995, task loss: 0.013543249107897282, entropy loss: 0.23350206017494202\n",
      "global step: 3200/5000, proportion: 0.4894111454486847, task loss: 0.015120471827685833, entropy loss: 0.23157060146331787\n",
      "global step: 3300/5000, proportion: 0.4941155016422272, task loss: 0.02608666568994522, entropy loss: 0.2296082079410553\n",
      "global step: 3400/5000, proportion: 0.4998458921909332, task loss: 0.0003800883423537016, entropy loss: 0.22759272158145905\n",
      "global step: 3500/5000, proportion: 0.47777432203292847, task loss: 0.00797278806567192, entropy loss: 0.2261161357164383\n",
      "global step: 3600/5000, proportion: 0.4801837205886841, task loss: 0.011102003045380116, entropy loss: 0.22389143705368042\n",
      "global step: 3700/5000, proportion: 0.48302629590034485, task loss: 0.006963240448385477, entropy loss: 0.22133246064186096\n",
      "global step: 3800/5000, proportion: 0.48587507009506226, task loss: 0.006033018697053194, entropy loss: 0.21836689114570618\n",
      "global step: 3900/5000, proportion: 0.48899102210998535, task loss: 0.013600971549749374, entropy loss: 0.21482151746749878\n",
      "global step: 4000/5000, proportion: 0.4924030900001526, task loss: 0.04906655475497246, entropy loss: 0.2105007916688919\n",
      "global step: 4100/5000, proportion: 0.4951477348804474, task loss: 0.0047930460423231125, entropy loss: 0.20534753799438477\n",
      "global step: 4200/5000, proportion: 0.49773311614990234, task loss: 0.006869711447507143, entropy loss: 0.1992821991443634\n",
      "global step: 4300/5000, proportion: 0.4997302293777466, task loss: 0.0006950889364816248, entropy loss: 0.19220542907714844\n",
      "global step: 4400/5000, proportion: 0.4730004072189331, task loss: 0.017367620021104813, entropy loss: 0.18404029309749603\n",
      "global step: 4500/5000, proportion: 0.4693625867366791, task loss: 0.0020726160146296024, entropy loss: 0.17544181644916534\n",
      "global step: 4600/5000, proportion: 0.46425101161003113, task loss: 0.029101278632879257, entropy loss: 0.16608162224292755\n",
      "global step: 4700/5000, proportion: 0.4585384726524353, task loss: 0.010903928428888321, entropy loss: 0.15626783668994904\n",
      "global step: 4800/5000, proportion: 0.45105940103530884, task loss: 0.0037862583994865417, entropy loss: 0.1459108144044876\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = pruning_model(images)\n",
    "\n",
    "        proportion, flops_loss_value, entropy_loss_value, bkd_loss_value = \\\n",
    "            pruning_config.compute_current_proportion_and_pruning_losses(global_step=global_step)\n",
    "        task_loss = nn.functional.nll_loss(outputs, labels)\n",
    "        loss = task_loss + flops_loss_value + entropy_loss_value + bkd_loss_value\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step % 100 == 0:\n",
    "            print(f'global step: {global_step}/{NUM_PRUNING_STEPS}, proportion: {proportion}, task loss: {task_loss}, entropy loss: {entropy_loss_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b219ccbc-444e-490b-8518-b4c63c4451e0",
   "metadata": {},
   "source": [
    "## 7. Remove channels from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbb0caa2-cde0-4a04-af15-a82e1151268e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv conv1: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv conv2: leaving fraction: 0.46875 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv conv3: leaving fraction: 1.0 of out channels.\n",
      "/opt/devine/torch-dag/torch_dag_algorithms/pruning/modules.py:221: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return [torch.tensor(scores_)]\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv fc1: leaving fraction: 0.1171875 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv fc2: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.channel_removal_primitives:Pruning conv fc3: leaving fraction: 1.0 of out channels.\n",
      "INFO:torch_dag_algorithms.pruning.mask_propagation:No explicit mask propagation for vertex: log_softmax, of type <class 'torch.nn.modules.activation.LogSoftmax'>. Returning `None` masks.\n"
     ]
    }
   ],
   "source": [
    "pruned_model = pruning_config.remove_channels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "019ad668-6a59-487a-9616-3bd460d35cb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_normalized_flops = tda.pruning.compute_normalized_flops(pruned_model, input_shape_without_batch=INPUT_SHAPE[1:])\n",
    "final_proportion = final_normalized_flops / initial_normalized_flops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e7e18b-1e51-4c10-a434-10833c51355a",
   "metadata": {},
   "source": [
    "## 8. Report accuracy and model size after pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36af7d08-c56f-45ec-8bed-73b4d341f3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the pruned model on the 10000 test images: 99.13 %\n"
     ]
    }
   ],
   "source": [
    "pruned_model.to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        outputs = pruned_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the pruned model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47443c47-1c37-40a4-8ed8-de450c1861f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial normalized flops: 21.232244897959184, final normalized flops: 9.596017857142858\n",
      "Final proportion: 0.45195493473538517\n"
     ]
    }
   ],
   "source": [
    "print(f'Initial normalized flops: {initial_normalized_flops}, final normalized flops: {final_normalized_flops}')\n",
    "print(f'Final proportion: {final_proportion}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd980498-571c-406a-bd71-3f84f2559704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
